{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSDTaaunTlL6o1G1YeUmMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rawatinder1/learning_tensorFlow/blob/main/00_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this noteBook , we are going to cover some of the most fundamental concepts of tensors using TensorFlow."
      ],
      "metadata": {
        "id": "U-lbWM8TWyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Tensors"
      ],
      "metadata": {
        "id": "pbWiEHH0XSmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import TensorFlow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJS70CpxXoOs",
        "outputId": "9232f564-c31a-40e6-bb73-11ad22bd8360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensors with tf.constant()\n",
        "scalar = tf.constant(7)\n",
        "scalar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxRpcoi_X1Os",
        "outputId": "afa26d96-3a9f-4509-a5eb-bd31d23b2db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=7>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#  1. Scalars (0D tensor)\n",
        "scalar = tf.constant(7)\n",
        "print(\"Scalar:\")\n",
        "print(scalar)\n",
        "print(\"Shape:\", scalar.shape, \"Rank:\", tf.rank(scalar).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  2. Vector (1D tensor)\n",
        "vector = tf.constant([1, 2, 3])\n",
        "print(\"Vector:\")\n",
        "print(vector)\n",
        "print(\"Shape:\", vector.shape, \"Rank:\", tf.rank(vector).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  3. Matrix (2D tensor)\n",
        "matrix = tf.constant([[1, 2, 3],\n",
        "                      [4, 5, 6]])\n",
        "print(\"Matrix:\")\n",
        "print(matrix)\n",
        "print(\"Shape:\", matrix.shape, \"Rank:\", tf.rank(matrix).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  4. 3D Tensor\n",
        "tensor3d = tf.constant([[[1, 2], [3, 4]],\n",
        "                        [[5, 6], [7, 8]]])\n",
        "print(\"3D Tensor:\")\n",
        "print(tensor3d)\n",
        "print(\"Shape:\", tensor3d.shape, \"Rank:\", tf.rank(tensor3d).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  5. Example: image-like 4D tensor (batch, height, width, channels)\n",
        "image_tensor = tf.random.uniform(shape=(4, 28, 28, 3))  # 4 RGB images, 28x28 each\n",
        "print(\"4D Tensor (like images):\")\n",
        "print(image_tensor)\n",
        "print(\"Shape:\", image_tensor.shape, \"Rank:\", tf.rank(image_tensor).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  6. Tensor properties\n",
        "print(\"Data type:\", image_tensor.dtype)\n",
        "print(\"Number of elements:\", tf.size(image_tensor).numpy())\n",
        "\n",
        "#  Summary\n",
        "print(\"\"\"\n",
        " Tensors are:\n",
        "- Multi-dimensional arrays (scalars, vectors, matrices, higher-D)\n",
        "- Defined by rank (number of dimensions) and shape\n",
        "- Backbone of deep learning data & model weights\n",
        "- GPU/TPU-compatible and support automatic differentiation\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "Gplf1Qq6iO-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a multidimensional tensor;\n",
        "tensor = tf.constant(7,shape=(3,2,3,3,2),\n",
        "dtype=tf.float32)\n",
        "\n",
        "print(tensor.ndim);\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HVXNGuLfMQ-",
        "outputId": "36c3cfef-5393-46f7-c9b0-252b124bbc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 3, 3, 2), dtype=float32, numpy=\n",
              "array([[[[[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]]],\n",
              "\n",
              "\n",
              "        [[[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]]],\n",
              "\n",
              "\n",
              "        [[[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]]],\n",
              "\n",
              "\n",
              "        [[[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]],\n",
              "\n",
              "         [[7., 7.],\n",
              "          [7., 7.],\n",
              "          [7., 7.]]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1=tf.constant([6,7])\n",
        "tensor2=tf.constant([9,8])\n",
        "result=tensor1*tensor2;\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TlQZfy4kDmT",
        "outputId": "7baa23a5-c337-42ea-fc90-6e40aa35d3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([54 56], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## creating tensors with `tf.variable()`"
      ],
      "metadata": {
        "id": "vcSZDU3zn5pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.variable is the mutable counterpart of tf.constant..\n",
        "#  tf.Variable(\n",
        "#     initial_value,\n",
        "#     trainable=True,\n",
        "#     dtype=None,\n",
        "#     shape=None,\n",
        "#     name=None\n",
        "# )\n",
        "# Key parameters:\n",
        "\n",
        "# initial_value – The starting value (can be a number, list, NumPy array, or another tensor).\n",
        "\n",
        "# trainable (default=True) – If True, TensorFlow will update it during training (e.g., by gradient descent).\n",
        "\n",
        "# dtype – Data type (inferred if not given).\n",
        "\n",
        "# shape – Shape of the variable (inferred if not given).\n",
        "\n",
        "# name – Optional name.\n",
        "\n",
        "mat = tf.Variable(tf.ones((2, 3)) * 7, dtype=tf.int32)\n",
        "mat[1,2].assign(52)\n",
        "mat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeDgYkkWoDu8",
        "outputId": "da0a6cd1-fd28-4ef8-e057-2dd8052d0919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
              "array([[ 7.,  7.,  7.],\n",
              "       [ 7.,  7., 52.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "changeable_tensor=tf.Variable(trainable=True,dtype=tf.float32,initial_value=tf.ones((2,3))*7)\n",
        "changeable_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiIGk0-vtfC3",
        "outputId": "07230eb3-9f59-4fb0-ead3-4b1d1b74e6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
              "array([[7., 7., 7.],\n",
              "       [7., 7., 7.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = tf.random.Generator.from_seed(42)\n",
        "\n",
        "random_variable=g.truncated_normal(shape=(3,2),mean=5.0,stddev=1.0)\n",
        "trainable_variable=tf.Variable(random_variable,trainable=True)\n",
        "print(trainable_variable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oJmJbXHenRX",
        "outputId": "c65c9a62-74cd-4426-b25b-a503b5780758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
            "array([[4.2434196, 4.9314528],\n",
            "       [5.07595  , 3.7426157],\n",
            "       [5.7093353, 3.8071513]], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## they are different ways you can create random tensors some of them are:->\n",
        "# using uniform distribution this will randomly generate numbers using a unifrm distribution or rectagular distribution the probability of selecting any number between a range [a,b] is fixed.\n",
        "random_tensor=tf.random.Generator.from_seed(42); # set seed for reproducibility.\n",
        "trainable_tensor=random_tensor.uniform(shape=(3,2),minval=0,maxval=100,dtype=tf.int32) # generate only int data type values for tensor;\n",
        "print(trainable_tensor);\n",
        "# using normal distributio or gaussian where probability is maximum around mean and decays exponentially in every dimensions that moves away from that mean value.\n",
        "trainable_tensor2=random_tensor.normal(shape=(3,2),mean=5.0,stddev=1.0)\n",
        "print(trainable_tensor2);\n",
        "# using trancated normal thats same as gaussian but outputs are truncated if theu are away from mean by +- 2*stddev.\n",
        "trainable_tensor3=random_tensor.truncated_normal(shape=(3,2),mean=5.0,stddev=1.0)\n",
        "print(trainable_tensor3);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc4aIsGagKfa",
        "outputId": "83487ea2-013a-4496-8d82-19a4046943dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 3 97]\n",
            " [26 98]\n",
            " [26 81]], shape=(3, 2), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[5.1752267 5.7110553]\n",
            " [5.5488243 5.14896  ]\n",
            " [4.45242   5.6163435]], shape=(3, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[6.286623  4.285244 ]\n",
            " [4.898045  5.3831787]\n",
            " [5.3264775 5.17025  ]], shape=(3, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##shuffle order of elements in a tensor\n",
        "> It looks like if you want the same order for your shuffled tensor you should set the global seed not the operation level seed\n"
      ],
      "metadata": {
        "id": "xOTQH6PlmoBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## shuffle a tensor(valuable for when you want to shuffle your data so that the inherent order doesnt effect learning )\n",
        "# Shuffling the order of elements in a tensor is a valuable technique in machine learning for several reasons:\n",
        "\n",
        "# Preventing bias: In machine learning, the order of data can sometimes influence the learning process, leading to biased models. Shuffling helps to break any inherent order or patterns in the data, ensuring that the model doesn't learn from the order itself.\n",
        "# Improving generalization: By presenting data in a random order, shuffling helps the model to generalize better to unseen data. It prevents the model from memorizing the training data's order and encourages it to learn the underlying patterns.\n",
        "# Ensuring independent and identically distributed (IID) data: Many machine learning algorithms assume that the data is IID. Shuffling helps to approximate this assumption by making the data points more independent of each other's position in the dataset.\n",
        "# Better convergence: In some optimization algorithms used for training models, shuffling the data can lead to faster and more stable convergence.\n",
        "# Essentially, shuffling helps to make the training process more robust and leads to better-performing models.\n",
        "\n",
        "import tensorflow as tf\n",
        "not_shuffled=tf.constant([[10,7],\n",
        "                          [3,4],\n",
        "                          [2,5]])\n",
        "tf.random.set_seed(42); # global level seed\n",
        "shuffled=tf.random.shuffle(not_shuffled ,seed=42) # operation level seed\n",
        "print(shuffled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGrVn3iXmtPJ",
        "outputId": "de93af1b-2cb4-46f9-c1d6-0f0ea17d2dc3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 3  4]\n",
            " [ 2  5]\n",
            " [10  7]], shape=(3, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Turning numpy arrays into tensors\n",
        " > NumPy arrays and TensorFlow tensors are fundamental data structures. While they can look similar and you can convert between them (as shown in your notebook), there are some key differences:\n",
        "\n",
        " > GPU/TPU Acceleration: TensorFlow tensors can be seamlessly moved to and run on GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units) for significant speedups in numerical computations, especially for large-scale deep learning tasks. NumPy operations primarily run on the CPU.\n",
        "\n",
        "\n",
        " > Automatic Differentiation: TensorFlow tensors are designed to work with TensorFlow's automatic differentiation system. This is crucial for training neural networks, where gradients need to be calculated efficiently. NumPy does not have built-in automatic differentiation capabilities.\n",
        "\n",
        "\n",
        " > Immutability vs. Mutability: By default, tf.constant tensors are immutable (their values cannot be changed after creation), whereas tf.Variable tensors are mutable. NumPy arrays are generally mutable.\n",
        "\n",
        "\n",
        " > Distributed Computing: TensorFlow is built to handle distributed computing, allowing you to run computations across multiple machines or devices. NumPy is primarily for single-machine processing.\n",
        "\n",
        "\n",
        " > Use Case: While NumPy is a general-purpose library for numerical computing and is excellent for tasks like data analysis and scientific computing, TensorFlow tensors are specifically designed and optimized for building and running deep learning models.\n",
        "\n",
        "\n",
        " > In summary, while you can perform similar operations on both, TensorFlow tensors are specifically engineered for the demands of deep learning, offering acceleration, automatic differentiation, and scalability that are not inherent in NumPy arrays.\n",
        "\n"
      ],
      "metadata": {
        "id": "3XcDkbGUsNkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "array=np.arange(1,25,dtype=np.int32)\n",
        "print(array);\n",
        "# try experimenting with shapes make sure dimproduct doesnt exceed the total number of\n",
        "# elements in the numpy array.\n",
        "A=tf.constant(array,shape=(2,3,2,2))\n",
        "print(\"Dimensions of tensor A : \" , A.ndim)\n",
        "print(A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAitb70ps1Xs",
        "outputId": "94badaed-0d9a-4ee2-b6ad-28790a6c1766"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
            "Dimensions of tensor A :  4\n",
            "tf.Tensor(\n",
            "[[[[ 1  2]\n",
            "   [ 3  4]]\n",
            "\n",
            "  [[ 5  6]\n",
            "   [ 7  8]]\n",
            "\n",
            "  [[ 9 10]\n",
            "   [11 12]]]\n",
            "\n",
            "\n",
            " [[[13 14]\n",
            "   [15 16]]\n",
            "\n",
            "  [[17 18]\n",
            "   [19 20]]\n",
            "\n",
            "  [[21 22]\n",
            "   [23 24]]]], shape=(2, 3, 2, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting information from our Tensor"
      ],
      "metadata": {
        "id": "NzvnEfM0w9Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# * Shape -> the length (number of elements) of each of the dimension of a tensor.\n",
        "# * Rank  -> the number of tensor dimensions.(tensor.ndim)\n",
        "# * Axis or Dimension -> A particular dimenion of a tensor\n",
        "# * Size -> The total number of items in the tensor\n",
        "import tensorflow as tf;\n",
        "def getInfo(tensor):\n",
        "  print(tensor)\n",
        "  print(\"size : \", tf.size(tensor).numpy())\n",
        "  print(\"Rank : \", tensor.ndim)\n",
        "  print(\"Shape : \", tensor.shape)\n",
        "  print(\"axis 0 : \", tensor[0]);\n",
        "  print(\"number of elements on 0th axis\",tensor.shape[0])\n",
        "  print(\"number of elements on last axis\",tensor.shape[-1])\n",
        "\n",
        "tensor=tf.constant([[1,2],\n",
        "                    [5,7],\n",
        "                    [5,9]]);\n",
        "getInfo(tensor)\n",
        "\n",
        "  # The shape (3, 2) tells us that the tensor has 2 dimensions.\n",
        "  # The first number in the shape, 3, represents the number of elements along the 0th axis (often thought of as rows).\n",
        "  # The second number in the shape, 2, represents the number of elements along the 1st axis (often thought of as columns).\n",
        "  # So, for your tensor tf.constant([[1,2],[5,7],[5,9]]):\n",
        "\n",
        "  # Along the 0th axis, you have three \"groups\" or \"rows\": [1, 2], [5, 7], and [5, 9]. That's why there are 3 elements on the 0th axis.\n",
        "  # Along the 1st axis, within each of those groups, you have two elements: 1 and 2, 5 and 7, 5 and 9. That's why there are 2 elements on the 1st axis.\n",
        "  # The output \"number of elements on 0th axis 3\" is correct based on the tensor's shape of (3, 2).\n",
        "\n",
        "  # Does that explanation help clarify how the shape relates to the number of elements on each axis?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGWJ1RgCxG-3",
        "outputId": "d1c253b0-e3a6-4a67-b98f-a69dc7394daf"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1 2]\n",
            " [5 7]\n",
            " [5 9]], shape=(3, 2), dtype=int32)\n",
            "size :  6\n",
            "Rank :  2\n",
            "Shape :  (3, 2)\n",
            "axis 0 :  tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
            "number of elements on 0th axis 3\n",
            "number of elements on last axis 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ee3f97"
      },
      "source": [
        "## Indexing in Python Lists and TensorFlow Tensors\n",
        "\n",
        "Both Python lists and TensorFlow tensors support indexing, which allows you to access individual elements or subsets of elements. While the syntax can be similar, it's important to understand the nuances of each.\n",
        "\n",
        "**Python Lists:**\n",
        "\n",
        "*   Lists are ordered collections of items.\n",
        "*   Indexing is used to access elements based on their position (starting from 0).\n",
        "*   You can use single indices, slicing (`[start:stop:step]`), and negative indices.\n",
        "\n",
        "**TensorFlow Tensors:**\n",
        "\n",
        "*   Tensors are multi-dimensional arrays.\n",
        "*   Indexing is used to access elements or slices along each dimension.\n",
        "*   Similar to NumPy arrays, you can use single indices, slicing, and advanced indexing techniques.\n",
        "*   Key difference: When slicing tensors, the resulting tensor retains its rank unless you explicitly squeeze a dimension of size 1.\n",
        "\n",
        "Let's look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "074f941d",
        "outputId": "aee5c8d2-1fa9-436d-c07b-c2665b2e1169"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Python List\n",
        "my_list = [10, 20, 30, 40, 50]\n",
        "\n",
        "# Accessing elements in a list\n",
        "print(\"Python List Indexing:\")\n",
        "print(\"First element:\", my_list[0])\n",
        "print(\"Third element:\", my_list[2])\n",
        "print(\"Last element:\", my_list[-1])\n",
        "print(\"Slice from index 1 to 3:\", my_list[1:4])\n",
        "print(\"Every other element:\", my_list[::2])\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# TensorFlow Tensor\n",
        "my_tensor = tf.constant([[1, 2, 3],\n",
        "                         [4, 5, 6],\n",
        "                         [7, 8, 9]])\n",
        "\n",
        "# Accessing elements in a tensor\n",
        "print(\"TensorFlow Tensor Indexing:\")\n",
        "print(\"First row:\", my_tensor[0])\n",
        "print(\"Third row:\", my_tensor[2])\n",
        "print(\"Last row:\", my_tensor[-1])\n",
        "print(\"Element at row 1, column 2:\", my_tensor[1, 2])\n",
        "print(\"Slice of rows from 0 to 1:\", my_tensor[0:2, :])\n",
        "print(\"Slice of columns from 1 onwards:\", my_tensor[:, 1:])\n",
        "print(\"Element at row 0, column 0:\", my_tensor[0, 0].numpy()) # Use .numpy() to get the scalar value\n",
        "\n",
        "# Similarities:\n",
        "# - Both use square brackets [] for indexing.\n",
        "# - Both support slicing with start:stop:step.\n",
        "# - Both support negative indexing to access elements from the end.\n",
        "\n",
        "# Differences:\n",
        "# - Tensors support multi-dimensional indexing (e.g., [row, column]).\n",
        "# - Slicing a tensor generally retains the original number of dimensions (rank)."
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python List Indexing:\n",
            "First element: 10\n",
            "Third element: 30\n",
            "Last element: 50\n",
            "Slice from index 1 to 3: [20, 30, 40]\n",
            "Every other element: [10, 30, 50]\n",
            "---------------------------------------------\n",
            "TensorFlow Tensor Indexing:\n",
            "First row: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
            "Third row: tf.Tensor([7 8 9], shape=(3,), dtype=int32)\n",
            "Last row: tf.Tensor([7 8 9], shape=(3,), dtype=int32)\n",
            "Element at row 1, column 2: tf.Tensor(6, shape=(), dtype=int32)\n",
            "Slice of rows from 0 to 1: tf.Tensor(\n",
            "[[1 2 3]\n",
            " [4 5 6]], shape=(2, 3), dtype=int32)\n",
            "Slice of columns from 1 onwards: tf.Tensor(\n",
            "[[2 3]\n",
            " [5 6]\n",
            " [8 9]], shape=(3, 2), dtype=int32)\n",
            "Element at row 0, column 0: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing in tensors"
      ],
      "metadata": {
        "id": "aTT0Og4ZB-pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_list=[2,2,5,7,2,10,4,2,4]\n",
        "print(my_list[:4]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMZy7ogQCDB7",
        "outputId": "f5d6da9a-4c27-44db-8dfa-a76a98ee514c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2, 5, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expanding tensor and operations on tensors\n",
        "> `+`,`-`,`*`,`\\`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ki319sdKCN9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_rank_4=tensor;\n",
        "tensor_rank_5=tf.expand_dims(tensor_rank_4,axis=-1)\n",
        "#tensor_rank_5=tensor[... , tf.newaxis]\n",
        "#print(tensor_rank_5.shape)\n",
        "print(tensor_rank_5)\n",
        "print(tf.multiply(tensor,10))\n",
        "print(tf.add(tensor,10000))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIUSxG0ULBr3",
        "outputId": "58a2197a-54b8-4f34-91ac-a388eb492778"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[[ 1]\n",
            "    [ 2]]\n",
            "\n",
            "   [[ 3]\n",
            "    [ 4]]\n",
            "\n",
            "   [[ 5]\n",
            "    [ 6]]]\n",
            "\n",
            "\n",
            "  [[[ 7]\n",
            "    [ 8]]\n",
            "\n",
            "   [[ 9]\n",
            "    [10]]\n",
            "\n",
            "   [[11]\n",
            "    [12]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[13]\n",
            "    [14]]\n",
            "\n",
            "   [[15]\n",
            "    [16]]\n",
            "\n",
            "   [[17]\n",
            "    [18]]]\n",
            "\n",
            "\n",
            "  [[[19]\n",
            "    [20]]\n",
            "\n",
            "   [[21]\n",
            "    [22]]\n",
            "\n",
            "   [[23]\n",
            "    [24]]]]], shape=(2, 2, 3, 2, 1), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[[[ 10  20]\n",
            "   [ 30  40]\n",
            "   [ 50  60]]\n",
            "\n",
            "  [[ 70  80]\n",
            "   [ 90 100]\n",
            "   [110 120]]]\n",
            "\n",
            "\n",
            " [[[130 140]\n",
            "   [150 160]\n",
            "   [170 180]]\n",
            "\n",
            "  [[190 200]\n",
            "   [210 220]\n",
            "   [230 240]]]], shape=(2, 2, 3, 2), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[[[10001 10002]\n",
            "   [10003 10004]\n",
            "   [10005 10006]]\n",
            "\n",
            "  [[10007 10008]\n",
            "   [10009 10010]\n",
            "   [10011 10012]]]\n",
            "\n",
            "\n",
            " [[[10013 10014]\n",
            "   [10015 10016]\n",
            "   [10017 10018]]\n",
            "\n",
            "  [[10019 10020]\n",
            "   [10021 10022]\n",
            "   [10023 10024]]]], shape=(2, 2, 3, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d158960"
      },
      "source": [
        "## Broadcasting in TensorFlow\n",
        "\n",
        "TensorFlow uses broadcasting to perform element-wise operations on tensors with different, but compatible, shapes. This allows operations like addition and division without requiring the tensors to have the exact same shape and size.\n",
        "\n",
        "When shapes are compatible, TensorFlow \"stretches\" or \"copies\" the smaller tensor along dimensions to match the shape of the larger tensor, without actually duplicating the data.\n",
        "\n",
        "**Broadcasting Rules (simplified):**\n",
        "\n",
        "1.  Compare shapes from right to left.\n",
        "2.  Dimensions are compatible if they are equal or one of them is 1.\n",
        "3.  If a dimension is missing in one tensor, it's padded with a dimension of size 1 on the left.\n",
        "4.  If dimensions are incompatible, an error occurs.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "*   Adding a scalar to a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6c38704",
        "outputId": "0cedcea3-9547-4ef1-baee-ba8c176a6916"
      },
      "source": [
        "import tensorflow as tf\n",
        "tensor_A = tf.constant([[1], [2], [3]]) # Shape (3, 1)\n",
        "tensor_B = tf.constant([[10, 20, 30]]) # Shape (1, 3)\n",
        "result = tensor_A + tensor_B         # Result shape (3, 3)\n",
        "print(result)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[11 21 31]\n",
            " [12 22 32]\n",
            " [13 23 33]], shape=(3, 3), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tensor_C=tf.constant(np.arange(1,13,dtype=np.int32),shape=(2,1,2,3));\n",
        "tensor_D=tf.constant(np.arange(1,145,dtype=np.int32),shape=(3,2,4,3,2));\n",
        "\n",
        "tensor_result=tf.matmul(tensor_C , tensor_D)\n",
        "tensor_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R280eC54YzMI",
        "outputId": "e343745e-1467-404b-fcbb-6ae463004e5e"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 4, 2, 2), dtype=int32, numpy=\n",
              "array([[[[[  22,   28],\n",
              "          [  49,   64]],\n",
              "\n",
              "         [[  58,   64],\n",
              "          [ 139,  154]],\n",
              "\n",
              "         [[  94,  100],\n",
              "          [ 229,  244]],\n",
              "\n",
              "         [[ 130,  136],\n",
              "          [ 319,  334]]],\n",
              "\n",
              "\n",
              "        [[[ 652,  676],\n",
              "          [ 895,  928]],\n",
              "\n",
              "         [[ 796,  820],\n",
              "          [1093, 1126]],\n",
              "\n",
              "         [[ 940,  964],\n",
              "          [1291, 1324]],\n",
              "\n",
              "         [[1084, 1108],\n",
              "          [1489, 1522]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[ 310,  316],\n",
              "          [ 769,  784]],\n",
              "\n",
              "         [[ 346,  352],\n",
              "          [ 859,  874]],\n",
              "\n",
              "         [[ 382,  388],\n",
              "          [ 949,  964]],\n",
              "\n",
              "         [[ 418,  424],\n",
              "          [1039, 1054]]],\n",
              "\n",
              "\n",
              "        [[[1804, 1828],\n",
              "          [2479, 2512]],\n",
              "\n",
              "         [[1948, 1972],\n",
              "          [2677, 2710]],\n",
              "\n",
              "         [[2092, 2116],\n",
              "          [2875, 2908]],\n",
              "\n",
              "         [[2236, 2260],\n",
              "          [3073, 3106]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[ 598,  604],\n",
              "          [1489, 1504]],\n",
              "\n",
              "         [[ 634,  640],\n",
              "          [1579, 1594]],\n",
              "\n",
              "         [[ 670,  676],\n",
              "          [1669, 1684]],\n",
              "\n",
              "         [[ 706,  712],\n",
              "          [1759, 1774]]],\n",
              "\n",
              "\n",
              "        [[[2956, 2980],\n",
              "          [4063, 4096]],\n",
              "\n",
              "         [[3100, 3124],\n",
              "          [4261, 4294]],\n",
              "\n",
              "         [[3244, 3268],\n",
              "          [4459, 4492]],\n",
              "\n",
              "         [[3388, 3412],\n",
              "          [4657, 4690]]]]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e744cf1a"
      },
      "source": [
        "## Broadcasting with `tf.matmul`\n",
        "\n",
        "When using `tf.matmul` to perform matrix multiplication on tensors, TensorFlow supports broadcasting not only on the last two dimensions (the matrices being multiplied) but also on the dimensions *before* the last two. These preceding dimensions are often referred to as \"batch\" dimensions.\n",
        "\n",
        "This means you can perform matrix multiplication between tensors of different ranks, provided their shapes are compatible according to broadcasting rules for these batch dimensions, and the inner dimensions of the last two dimensions match for the matrix multiplication itself.\n",
        "\n",
        "**How `tf.matmul` Broadcasting Works:**\n",
        "\n",
        "1.  **Matrix Dimensions:** The last two dimensions of the tensors are treated as the matrices. For the operation to be valid, the inner dimensions of these matrices must match (the number of columns in the first matrix must equal the number of rows in the second matrix).\n",
        "2.  **Batch Dimensions:** The dimensions before the last two are considered batch dimensions. TensorFlow applies standard broadcasting rules to these batch dimensions from left to right.\n",
        "3.  **Result Shape:** The shape of the resulting tensor will have the broadcasted batch dimensions followed by the outer dimensions of the matrix multiplication result.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider `tensor_C` with shape `(2, 2, 2, 3)` (Rank 4) and `tensor_D` with shape `(1, 2, 1, 3, 2)` (Rank 5).\n",
        "\n",
        "*   **Matrix Dimensions:** `(2, 3)` from `tensor_C` and `(3, 2)` from `tensor_D`. The inner dimensions (3 and 3) match. The result of the matrix multiplication on these dimensions will have shape `(2, 2)`.\n",
        "*   **Batch Dimensions:** `(2, 2)` from `tensor_C` and `(1, 2, 1)` from `tensor_D`. Applying broadcasting rules:\n",
        "    *   Dimension 0: `2` vs `1` -> Broadcasts to `2`\n",
        "    *   Dimension 1: `2` vs `2` -> Compatible, result is `2`\n",
        "    *   Dimension 2 (from tensor_D only): `1` -> Broadcasts to `1`\n",
        "    *   The broadcasted batch shape is effectively `(2, 2, 1)`. This, combined with the matrix multiplication result shape `(2, 2)`, gives a final shape of `(2, 2, 1, 2, 2)`. The output shape `(1, 2, 2, 2, 2)` seen previously is likely due to the initial dimension of 1 in `tensor_D` influencing the broadcasted batch shape.\n",
        "\n",
        "The final result shape combines the broadcasted batch dimensions and the matrix multiplication result shape.\n",
        "\n",
        "This broadcasting capability in `tf.matmul` is powerful as it allows for efficient operations across batches of matrices without explicit loops.\n",
        "👇\n",
        "\n",
        "🧠 Note on tf.matmul broadcasting rules:\n",
        "\n",
        "✅ Inner dimensions must match exactly:\n",
        "The last dimension of A and the second-to-last of B must be equal.\n",
        "\n",
        "If A is (..., m, n) and B is (..., n, p) → result is (..., m, p)\n",
        "\n",
        "✅ Leading (batch) dimensions can broadcast, but only if they are either equal or 1.\n",
        "\n",
        "Example: (1, m, n) × (b, n, p) → (b, m, p) ✅\n",
        "\n",
        "Example: (b, m, n) × (b, n, p) → (b, m, p) ✅\n",
        "\n",
        "❌ If any leading dimension is different and neither is 1, broadcasting fails.\n",
        "\n",
        "Example: (b, m, n) × (c, n, p) → ❌ fails if b ≠ c and neither is 1.\n",
        "\n",
        "✅ If B has no batch dimension (e.g., shape (n, p)), it is treated as shared across all batches.\n",
        "\n",
        "Example: (b, m, n) × (n, p) → (b, m, p)\n",
        "\n",
        "\n",
        "\n",
        "🧠 Note – Why tf.matmul Worked With (2,2,2,3) × (3,2,1,3,2)\n",
        "\n",
        "Even though the shapes of the two tensors look incompatible, tf.matmul did not break its rules. Here’s why the operation succeeds:\n",
        "\n",
        "Inner dimensions must match\n",
        "\n",
        "Last two dims of tensor_C: (2, 3)\n",
        "\n",
        "Last two dims of tensor_D: (3, 2)\n",
        "✅ 3 == 3 → inner matmul is valid → (2, 3) × (3, 2) → (2, 2)\n",
        "\n",
        "Batch dimensions can broadcast\n",
        "\n",
        "tensor_C batch dims: (2, 2)\n",
        "\n",
        "tensor_D batch dims: (3, 2, 1)\n",
        "\n",
        "Aligning from right:\n",
        "\n",
        "2 vs 1 → ✅ 1 broadcasts to 2\n",
        "\n",
        "2 vs 2 → ✅ equal\n",
        "\n",
        "(missing) vs 3 → ✅ missing dims treated as 1, then broadcast to 3\n",
        "\n",
        "✅ Resulting batch shape: (3, 2, 2)\n",
        "\n",
        "Final result shape\n",
        "Combine broadcasted batch dims (3, 2, 2) with the matmul result (2, 2)\n",
        "→ Final tensor shape: (3, 2, 2, 2, 2)\n",
        "\n",
        "💡 Key Takeaways:\n",
        "\n",
        "TensorFlow treats missing batch dimensions as 1, allowing broadcasting.\n",
        "\n",
        "A 1 in any batch dimension makes a tensor more flexible — it can “expand” to match larger shapes.\n",
        "\n",
        "The matmul rule ((..., m, n) × (..., n, p) → (..., m, p)) is always respected — this example just shows broadcasting at work."
      ]
    }
  ]
}
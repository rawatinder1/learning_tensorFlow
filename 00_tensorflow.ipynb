{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5EgQYDgh5JMGypxTgu7tc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rawatinder1/learning_tensorFlow/blob/main/00_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this noteBook , we are going to cover some of the most fundamental concepts of tensors using TensorFlow."
      ],
      "metadata": {
        "id": "U-lbWM8TWyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Tensors"
      ],
      "metadata": {
        "id": "pbWiEHH0XSmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import TensorFlow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__);"
      ],
      "metadata": {
        "id": "AJS70CpxXoOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensors with tf.constant()\n",
        "scalar = tf.constant(7)\n",
        "scalar"
      ],
      "metadata": {
        "id": "NxRpcoi_X1Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#  1. Scalars (0D tensor)\n",
        "scalar = tf.constant(7)\n",
        "print(\"Scalar:\")\n",
        "print(scalar)\n",
        "print(\"Shape:\", scalar.shape, \"Rank:\", tf.rank(scalar).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  2. Vector (1D tensor)\n",
        "vector = tf.constant([1, 2, 3])\n",
        "print(\"Vector:\")\n",
        "print(vector)\n",
        "print(\"Shape:\", vector.shape, \"Rank:\", tf.rank(vector).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  3. Matrix (2D tensor)\n",
        "matrix = tf.constant([[1, 2, 3],\n",
        "                      [4, 5, 6]])\n",
        "print(\"Matrix:\")\n",
        "print(matrix)\n",
        "print(\"Shape:\", matrix.shape, \"Rank:\", tf.rank(matrix).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  4. 3D Tensor\n",
        "tensor3d = tf.constant([[[1, 2], [3, 4]],\n",
        "                        [[5, 6], [7, 8]]])\n",
        "print(\"3D Tensor:\")\n",
        "print(tensor3d)\n",
        "print(\"Shape:\", tensor3d.shape, \"Rank:\", tf.rank(tensor3d).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  5. Example: image-like 4D tensor (batch, height, width, channels)\n",
        "image_tensor = tf.random.uniform(shape=(4, 28, 28, 3))  # 4 RGB images, 28x28 each\n",
        "print(\"4D Tensor (like images):\")\n",
        "print(image_tensor)\n",
        "print(\"Shape:\", image_tensor.shape, \"Rank:\", tf.rank(image_tensor).numpy())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  6. Tensor properties\n",
        "print(\"Data type:\", image_tensor.dtype)\n",
        "print(\"Number of elements:\", tf.size(image_tensor).numpy())\n",
        "\n",
        "#  Summary\n",
        "print(\"\"\"\n",
        " Tensors are:\n",
        "- Multi-dimensional arrays (scalars, vectors, matrices, higher-D)\n",
        "- Defined by rank (number of dimensions) and shape\n",
        "- Backbone of deep learning data & model weights\n",
        "- GPU/TPU-compatible and support automatic differentiation\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "Gplf1Qq6iO-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a multidimensional tensor;\n",
        "tensor = tf.constant(7,shape=(3,2,3,3,2),\n",
        "dtype=tf.float32)\n",
        "\n",
        "print(tensor.ndim);\n",
        "tensor"
      ],
      "metadata": {
        "id": "9HVXNGuLfMQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1=tf.constant([6,7])\n",
        "tensor2=tf.constant([9,8])\n",
        "result=tensor1*tensor2;\n",
        "print(result)"
      ],
      "metadata": {
        "id": "-TlQZfy4kDmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## creating tensors with `tf.variable()`"
      ],
      "metadata": {
        "id": "vcSZDU3zn5pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.variable is the mutable counterpart of tf.constant..\n",
        "#  tf.Variable(\n",
        "#     initial_value,\n",
        "#     trainable=True,\n",
        "#     dtype=None,\n",
        "#     shape=None,\n",
        "#     name=None\n",
        "# )\n",
        "# Key parameters:\n",
        "\n",
        "# initial_value – The starting value (can be a number, list, NumPy array, or another tensor).\n",
        "\n",
        "# trainable (default=True) – If True, TensorFlow will update it during training (e.g., by gradient descent).\n",
        "\n",
        "# dtype – Data type (inferred if not given).\n",
        "\n",
        "# shape – Shape of the variable (inferred if not given).\n",
        "\n",
        "# name – Optional name.\n",
        "\n",
        "mat = tf.Variable(tf.ones((2, 3)) * 7, dtype=tf.float32)\n",
        "mat[1,2].assign(52)\n",
        "mat\n"
      ],
      "metadata": {
        "id": "LeDgYkkWoDu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JswiWBt9Pw1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "changeable_tensor=tf.Variable(trainable=True,dtype=tf.float32,initial_value=tf.ones((2,3))*7)\n",
        "changeable_tensor"
      ],
      "metadata": {
        "id": "iiIGk0-vtfC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = tf.random.Generator.from_seed(42)\n",
        "\n",
        "random_variable=g.truncated_normal(shape=(3,2),mean=5.0,stddev=1.0)\n",
        "trainable_variable=tf.Variable(random_variable,trainable=True)\n",
        "print(trainable_variable)"
      ],
      "metadata": {
        "id": "7oJmJbXHenRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## they are different ways you can create random tensors some of them are:->\n",
        "# using uniform distribution this will randomly generate numbers using a unifrm distribution or rectagular distribution the probability of selecting any number between a range [a,b] is fixed.\n",
        "random_tensor=tf.random.Generator.from_seed(42); # set seed for reproducibility.\n",
        "trainable_tensor=random_tensor.uniform(shape=(3,2),minval=0,maxval=100,dtype=tf.int32) # generate only int data type values for tensor;\n",
        "print(trainable_tensor);\n",
        "# using normal distributio or gaussian where probability is maximum around mean and decays exponentially in every dimensions that moves away from that mean value.\n",
        "trainable_tensor2=random_tensor.normal(shape=(3,2),mean=5.0,stddev=1.0)\n",
        "print(trainable_tensor2);\n",
        "# using trancated normal thats same as gaussian but outputs are truncated if theu are away from mean by +- 2*stddev.\n",
        "trainable_tensor3=random_tensor.truncated_normal(shape=(3,2),mean=5.0,stddev=1.0)\n",
        "print(trainable_tensor3);\n"
      ],
      "metadata": {
        "id": "xc4aIsGagKfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##shuffle order of elements in a tensor\n",
        "> It looks like if you want the same order for your shuffled tensor you should set the global seed not the operation level seed\n"
      ],
      "metadata": {
        "id": "xOTQH6PlmoBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## shuffle a tensor(valuable for when you want to shuffle your data so that the inherent order doesnt effect learning )\n",
        "# Shuffling the order of elements in a tensor is a valuable technique in machine learning for several reasons:\n",
        "\n",
        "# Preventing bias: In machine learning, the order of data can sometimes influence the learning process, leading to biased models. Shuffling helps to break any inherent order or patterns in the data, ensuring that the model doesn't learn from the order itself.\n",
        "# Improving generalization: By presenting data in a random order, shuffling helps the model to generalize better to unseen data. It prevents the model from memorizing the training data's order and encourages it to learn the underlying patterns.\n",
        "# Ensuring independent and identically distributed (IID) data: Many machine learning algorithms assume that the data is IID. Shuffling helps to approximate this assumption by making the data points more independent of each other's position in the dataset.\n",
        "# Better convergence: In some optimization algorithms used for training models, shuffling the data can lead to faster and more stable convergence.\n",
        "# Essentially, shuffling helps to make the training process more robust and leads to better-performing models.\n",
        "\n",
        "import tensorflow as tf\n",
        "not_shuffled=tf.constant([[10,7],\n",
        "                          [3,4],\n",
        "                          [2,5]])\n",
        "tf.random.set_seed(42); # global level seed\n",
        "shuffled=tf.random.shuffle(not_shuffled ,seed=42) # operation level seed\n",
        "print(shuffled)\n",
        "\n"
      ],
      "metadata": {
        "id": "xGrVn3iXmtPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Turning numpy arrays into tensors\n",
        " > NumPy arrays and TensorFlow tensors are fundamental data structures. While they can look similar and you can convert between them (as shown in your notebook), there are some key differences:\n",
        "\n",
        " > GPU/TPU Acceleration: TensorFlow tensors can be seamlessly moved to and run on GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units) for significant speedups in numerical computations, especially for large-scale deep learning tasks. NumPy operations primarily run on the CPU.\n",
        "\n",
        "\n",
        " > Automatic Differentiation: TensorFlow tensors are designed to work with TensorFlow's automatic differentiation system. This is crucial for training neural networks, where gradients need to be calculated efficiently. NumPy does not have built-in automatic differentiation capabilities.\n",
        "\n",
        "\n",
        " > Immutability vs. Mutability: By default, tf.constant tensors are immutable (their values cannot be changed after creation), whereas tf.Variable tensors are mutable. NumPy arrays are generally mutable.\n",
        "\n",
        "\n",
        " > Distributed Computing: TensorFlow is built to handle distributed computing, allowing you to run computations across multiple machines or devices. NumPy is primarily for single-machine processing.\n",
        "\n",
        "\n",
        " > Use Case: While NumPy is a general-purpose library for numerical computing and is excellent for tasks like data analysis and scientific computing, TensorFlow tensors are specifically designed and optimized for building and running deep learning models.\n",
        "\n",
        "\n",
        " > In summary, while you can perform similar operations on both, TensorFlow tensors are specifically engineered for the demands of deep learning, offering acceleration, automatic differentiation, and scalability that are not inherent in NumPy arrays.\n",
        "\n"
      ],
      "metadata": {
        "id": "3XcDkbGUsNkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "array=np.arange(1,25,dtype=np.int32)\n",
        "print(array);\n",
        "# try experimenting with shapes make sure dimproduct doesnt exceed the total number of\n",
        "# elements in the numpy array.\n",
        "A=tf.constant(array,shape=(2,3,2,2))\n",
        "print(\"Dimensions of tensor A : \" , A.ndim)\n",
        "print(A)\n"
      ],
      "metadata": {
        "id": "wAitb70ps1Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting information from our Tensor"
      ],
      "metadata": {
        "id": "NzvnEfM0w9Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# * Shape -> the length (number of elements) of each of the dimension of a tensor.\n",
        "# * Rank  -> the number of tensor dimensions.(tensor.ndim)\n",
        "# * Axis or Dimension -> A particular dimenion of a tensor\n",
        "# * Size -> The total number of items in the tensor\n",
        "import tensorflow as tf;\n",
        "def getInfo(tensor):\n",
        "  print(tensor)\n",
        "  print(\"size : \", tf.size(tensor).numpy())\n",
        "  print(\"Rank : \", tensor.ndim)\n",
        "  print(\"Shape : \", tensor.shape)\n",
        "  print(\"axis 0 : \", tensor[0]);\n",
        "  print(\"number of elements on 0th axis\",tensor.shape[0])\n",
        "  print(\"number of elements on last axis\",tensor.shape[-1])\n",
        "\n",
        "tensor=tf.constant([[1,2],\n",
        "                    [5,7],\n",
        "                    [5,9]]);\n",
        "getInfo(tensor)\n",
        "\n",
        "  # The shape (3, 2) tells us that the tensor has 2 dimensions.\n",
        "  # The first number in the shape, 3, represents the number of elements along the 0th axis (often thought of as rows).\n",
        "  # The second number in the shape, 2, represents the number of elements along the 1st axis (often thought of as columns).\n",
        "  # So, for your tensor tf.constant([[1,2],[5,7],[5,9]]):\n",
        "\n",
        "  # Along the 0th axis, you have three \"groups\" or \"rows\": [1, 2], [5, 7], and [5, 9]. That's why there are 3 elements on the 0th axis.\n",
        "  # Along the 1st axis, within each of those groups, you have two elements: 1 and 2, 5 and 7, 5 and 9. That's why there are 2 elements on the 1st axis.\n",
        "  # The output \"number of elements on 0th axis 3\" is correct based on the tensor's shape of (3, 2).\n",
        "\n",
        "  # Does that explanation help clarify how the shape relates to the number of elements on each axis?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DGWJ1RgCxG-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ee3f97"
      },
      "source": [
        "## Indexing in Python Lists and TensorFlow Tensors\n",
        "\n",
        "Both Python lists and TensorFlow tensors support indexing, which allows you to access individual elements or subsets of elements. While the syntax can be similar, it's important to understand the nuances of each.\n",
        "\n",
        "**Python Lists:**\n",
        "\n",
        "*   Lists are ordered collections of items.\n",
        "*   Indexing is used to access elements based on their position (starting from 0).\n",
        "*   You can use single indices, slicing (`[start:stop:step]`), and negative indices.\n",
        "\n",
        "**TensorFlow Tensors:**\n",
        "\n",
        "*   Tensors are multi-dimensional arrays.\n",
        "*   Indexing is used to access elements or slices along each dimension.\n",
        "*   Similar to NumPy arrays, you can use single indices, slicing, and advanced indexing techniques.\n",
        "*   Key difference: When slicing tensors, the resulting tensor retains its rank unless you explicitly squeeze a dimension of size 1.\n",
        "\n",
        "Let's look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "074f941d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Python List\n",
        "my_list = [10, 20, 30, 40, 50]\n",
        "\n",
        "# Accessing elements in a list\n",
        "print(\"Python List Indexing:\")\n",
        "print(\"First element:\", my_list[0])\n",
        "print(\"Third element:\", my_list[2])\n",
        "print(\"Last element:\", my_list[-1])\n",
        "print(\"Slice from index 1 to 3:\", my_list[1:4])\n",
        "print(\"Every other element:\", my_list[::2])\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# TensorFlow Tensor\n",
        "my_tensor = tf.constant([[1, 2, 3],\n",
        "                         [4, 5, 6],\n",
        "                         [7, 8, 9]])\n",
        "\n",
        "# Accessing elements in a tensor\n",
        "print(\"TensorFlow Tensor Indexing:\")\n",
        "print(\"First row:\", my_tensor[0])\n",
        "print(\"Third row:\", my_tensor[2])\n",
        "print(\"Last row:\", my_tensor[-1])\n",
        "print(\"Element at row 1, column 2:\", my_tensor[1, 2])\n",
        "print(\"Slice of rows from 0 to 1:\", my_tensor[0:2, :])\n",
        "print(\"Slice of columns from 1 onwards:\", my_tensor[:, 1:])\n",
        "print(\"Element at row 0, column 0:\", my_tensor[0, 0].numpy()) # Use .numpy() to get the scalar value\n",
        "\n",
        "# Similarities:\n",
        "# - Both use square brackets [] for indexing.\n",
        "# - Both support slicing with start:stop:step.\n",
        "# - Both support negative indexing to access elements from the end.\n",
        "\n",
        "# Differences:\n",
        "# - Tensors support multi-dimensional indexing (e.g., [row, column]).\n",
        "# - Slicing a tensor generally retains the original number of dimensions (rank)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing in tensors"
      ],
      "metadata": {
        "id": "aTT0Og4ZB-pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_list=[2,2,5,7,2,10,4,2,4]\n",
        "print(my_list[:4]);"
      ],
      "metadata": {
        "id": "YMZy7ogQCDB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expanding tensor and operations on tensors\n",
        "> `+`,`-`,`*`,`\\`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ki319sdKCN9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_rank_4=tensor;\n",
        "tensor_rank_5=tf.expand_dims(tensor_rank_4,axis=-1)\n",
        "#tensor_rank_5=tensor[... , tf.newaxis]\n",
        "#print(tensor_rank_5.shape)\n",
        "print(tensor_rank_5)\n",
        "print(tf.multiply(tensor,10))\n",
        "print(tf.add(tensor,10000))\n"
      ],
      "metadata": {
        "id": "SIUSxG0ULBr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d158960"
      },
      "source": [
        "## Broadcasting in TensorFlow\n",
        "\n",
        "TensorFlow uses broadcasting to perform element-wise operations on tensors with different, but compatible, shapes. This allows operations like addition and division without requiring the tensors to have the exact same shape and size.\n",
        "\n",
        "When shapes are compatible, TensorFlow \"stretches\" or \"copies\" the smaller tensor along dimensions to match the shape of the larger tensor, without actually duplicating the data.\n",
        "\n",
        "**Broadcasting Rules (simplified):**\n",
        "\n",
        "1.  Compare shapes from right to left.\n",
        "2.  Dimensions are compatible if they are equal or one of them is 1.\n",
        "3.  If a dimension is missing in one tensor, it's padded with a dimension of size 1 on the left.\n",
        "4.  If dimensions are incompatible, an error occurs.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "*   Adding a scalar to a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6c38704"
      },
      "source": [
        "import tensorflow as tf\n",
        "tensor_A = tf.constant([[1], [2], [3]]) # Shape (3, 1)\n",
        "tensor_B = tf.constant([[10, 20, 30]]) # Shape (1, 3)\n",
        "result = tensor_A + tensor_B         # Result shape (3, 3)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tensor_C=tf.constant(np.arange(1,13,dtype=np.int32),shape=(2,1,2,3));\n",
        "tensor_D=tf.constant(np.arange(1,145,dtype=np.int32),shape=(3,2,4,3,2));\n",
        "\n",
        "tensor_result=tf.matmul(tensor_C , tensor_D)\n",
        "tensor_result"
      ],
      "metadata": {
        "id": "R280eC54YzMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e744cf1a"
      },
      "source": [
        "## Broadcasting with `tf.matmul`\n",
        "\n",
        "When using `tf.matmul` to perform matrix multiplication on tensors, TensorFlow supports broadcasting not only on the last two dimensions (the matrices being multiplied) but also on the dimensions *before* the last two. These preceding dimensions are often referred to as \"batch\" dimensions.\n",
        "\n",
        "This means you can perform matrix multiplication between tensors of different ranks, provided their shapes are compatible according to broadcasting rules for these batch dimensions, and the inner dimensions of the last two dimensions match for the matrix multiplication itself.\n",
        "\n",
        "**How `tf.matmul` Broadcasting Works:**\n",
        "\n",
        "1.  **Matrix Dimensions:** The last two dimensions of the tensors are treated as the matrices. For the operation to be valid, the inner dimensions of these matrices must match (the number of columns in the first matrix must equal the number of rows in the second matrix).\n",
        "2.  **Batch Dimensions:** The dimensions before the last two are considered batch dimensions. TensorFlow applies standard broadcasting rules to these batch dimensions from left to right.\n",
        "3.  **Result Shape:** The shape of the resulting tensor will have the broadcasted batch dimensions followed by the outer dimensions of the matrix multiplication result.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider `tensor_C` with shape `(2, 2, 2, 3)` (Rank 4) and `tensor_D` with shape `(1, 2, 1, 3, 2)` (Rank 5).\n",
        "\n",
        "*   **Matrix Dimensions:** `(2, 3)` from `tensor_C` and `(3, 2)` from `tensor_D`. The inner dimensions (3 and 3) match. The result of the matrix multiplication on these dimensions will have shape `(2, 2)`.\n",
        "*   **Batch Dimensions:** `(2, 2)` from `tensor_C` and `(1, 2, 1)` from `tensor_D`. Applying broadcasting rules:\n",
        "    *   Dimension 0: `2` vs `1` -> Broadcasts to `2`\n",
        "    *   Dimension 1: `2` vs `2` -> Compatible, result is `2`\n",
        "    *   Dimension 2 (from tensor_D only): `1` -> Broadcasts to `1`\n",
        "    *   The broadcasted batch shape is effectively `(2, 2, 1)`. This, combined with the matrix multiplication result shape `(2, 2)`, gives a final shape of `(2, 2, 1, 2, 2)`. The output shape `(1, 2, 2, 2, 2)` seen previously is likely due to the initial dimension of 1 in `tensor_D` influencing the broadcasted batch shape.\n",
        "\n",
        "The final result shape combines the broadcasted batch dimensions and the matrix multiplication result shape.\n",
        "\n",
        "This broadcasting capability in `tf.matmul` is powerful as it allows for efficient operations across batches of matrices without explicit loops.\n",
        "👇\n",
        "\n",
        "🧠 Note on tf.matmul broadcasting rules:\n",
        "\n",
        "✅ Inner dimensions must match exactly:\n",
        "The last dimension of A and the second-to-last of B must be equal.\n",
        "\n",
        "If A is (..., m, n) and B is (..., n, p) → result is (..., m, p)\n",
        "\n",
        "✅ Leading (batch) dimensions can broadcast, but only if they are either equal or 1.\n",
        "\n",
        "Example: (1, m, n) × (b, n, p) → (b, m, p) ✅\n",
        "\n",
        "Example: (b, m, n) × (b, n, p) → (b, m, p) ✅\n",
        "\n",
        "❌ If any leading dimension is different and neither is 1, broadcasting fails.\n",
        "\n",
        "Example: (b, m, n) × (c, n, p) → ❌ fails if b ≠ c and neither is 1.\n",
        "\n",
        "✅ If B has no batch dimension (e.g., shape (n, p)), it is treated as shared across all batches.\n",
        "\n",
        "Example: (b, m, n) × (n, p) → (b, m, p)\n",
        "\n",
        "\n",
        "\n",
        "🧠 Note – Why tf.matmul Worked With (2,2,2,3) × (3,2,1,3,2)\n",
        "\n",
        "Even though the shapes of the two tensors look incompatible, tf.matmul did not break its rules. Here’s why the operation succeeds:\n",
        "\n",
        "Inner dimensions must match\n",
        "\n",
        "Last two dims of tensor_C: (2, 3)\n",
        "\n",
        "Last two dims of tensor_D: (3, 2)\n",
        "✅ 3 == 3 → inner matmul is valid → (2, 3) × (3, 2) → (2, 2)\n",
        "\n",
        "Batch dimensions can broadcast\n",
        "\n",
        "tensor_C batch dims: (2, 2)\n",
        "\n",
        "tensor_D batch dims: (3, 2, 1)\n",
        "\n",
        "Aligning from right:\n",
        "\n",
        "2 vs 1 → ✅ 1 broadcasts to 2\n",
        "\n",
        "2 vs 2 → ✅ equal\n",
        "\n",
        "(missing) vs 3 → ✅ missing dims treated as 1, then broadcast to 3\n",
        "\n",
        "✅ Resulting batch shape: (3, 2, 2)\n",
        "\n",
        "Final result shape\n",
        "Combine broadcasted batch dims (3, 2, 2) with the matmul result (2, 2)\n",
        "→ Final tensor shape: (3, 2, 2, 2, 2)\n",
        "\n",
        "💡 Key Takeaways:\n",
        "\n",
        "TensorFlow treats missing batch dimensions as 1, allowing broadcasting.\n",
        "\n",
        "A 1 in any batch dimension makes a tensor more flexible — it can “expand” to match larger shapes.\n",
        "\n",
        "The matmul rule ((..., m, n) × (..., n, p) → (..., m, p)) is always respected — this example just shows broadcasting at work."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "djmkj_8Wh2eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tensor_A=tf.constant(np.arange(1,49,dtype=np.int32),shape=(2,2,2,3,2))\n",
        "#print(tensor_A)\n",
        "tensor_B = tf.transpose(tensor_A, perm=[0, 1, 2, 4, 3])\n",
        "# using perm you can specify exactly which axes you wanna reorder.\n",
        "\n",
        "print(tensor_B.shape)\n",
        "\n",
        "\n",
        "#print(tf.transpose(tensor_A))\n"
      ],
      "metadata": {
        "id": "w_jKL5c4r1jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa93dc02"
      },
      "source": [
        "## Tensor Transpose and Reshape in TensorFlow\n",
        "\n",
        "Understanding `tf.transpose()` and `tf.reshape()` is crucial for manipulating tensor dimensions effectively. While both change the arrangement of a tensor's elements, they do so in fundamentally different ways.\n",
        "\n",
        "### 1️⃣ `tf.transpose()` – Reordering Axes\n",
        "\n",
        "*   **Purpose:** Reorders the axes (dimensions) of a tensor according to a specified permutation.\n",
        "*   **Syntax:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75d28e5d"
      },
      "source": [
        "tf.transpose(tensor, perm=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba1bab7e"
      },
      "source": [
        "*   `tensor`: the input tensor\n",
        "*   `perm`: list specifying the desired order of axes\n",
        "*   **Default behavior:** If `perm=None`, it reverses all axes.\n",
        "\n",
        "### 2️⃣ Example – High-rank tensor\n",
        "\n",
        "Let's use a tensor with shape `(2, 2, 2, 3, 2)` to illustrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39167b12"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tensor_A = tf.constant(np.arange(1,49).reshape(2,2,2,3,2))\n",
        "print(\"Original tensor shape:\", tensor_A.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeb29cd6"
      },
      "source": [
        "Axes positions:\n",
        "\n",
        "| Axis | Meaning                   |\n",
        "| :--- | :------------------------ |\n",
        "| 0    | batch dim 1               |\n",
        "| 1    | batch dim 2               |\n",
        "| 2    | batch dim 3               |\n",
        "| 3    | rows of inner 2D matrix   |\n",
        "| 4    | columns of inner 2D matrix|\n",
        "\n",
        "Applying `tf.transpose()` with default `perm=None`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23b2ab0"
      },
      "source": [
        "tensor_B = tf.transpose(tensor_A)\n",
        "print(\"Shape after default transpose:\", tensor_B.shape)\n",
        "# Default tf.transpose reversed all axes: (0,1,2,3,4) → (4,3,2,1,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06c83699"
      },
      "source": [
        "This explains why the shape changed: the original axes `(0, 1, 2, 3, 4)` were reordered to `(4, 3, 2, 1, 0)`.\n",
        "\n",
        "### 3️⃣ Flipping only the last two dimensions\n",
        "\n",
        "Often, we want to transpose the inner 2D matrices without touching batch dimensions. Use the `perm` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e39352f0"
      },
      "source": [
        "tensor_C = tf.transpose(tensor_A, perm=[0, 1, 2, 4, 3])\n",
        "print(\"Shape after transposing last two axes:\", tensor_C.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbdcf60c"
      },
      "source": [
        "Here, `perm=[0, 1, 2, 4, 3]` means:\n",
        "\n",
        "*   `0, 1, 2`: The first three batch dimensions remain in their original positions.\n",
        "*   `4, 3`: The last two dimensions (rows and columns of the inner matrix) are swapped.\n",
        "\n",
        "✅ Only the 2D matrices (originally 3×2) are flipped to 2×3.\n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ `tf.reshape()` – Changing shape without changing data\n",
        "\n",
        "*   **Purpose:** Rearrange tensor into a new shape without altering the elements or their order in the flattened view.\n",
        "*   **Syntax:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5836dcb6"
      },
      "source": [
        "*   `tensor`: the input tensor\n",
        "*   `new_shape`: a list or tuple specifying the desired new shape. One dimension can be `-1`, which TensorFlow will calculate based on the total number of elements.\n",
        "*   **Key rules:**\n",
        "    *   The total number of elements must remain the same.\n",
        "    *   `product of original shape` = `product of new shape`\n",
        "\n",
        "*   Works independently of batch or matrix dimensions; it just repackages the existing elements into the new shape.\n",
        "\n",
        "### 5️⃣ When to use `transpose` vs `reshape`\n",
        "\n",
        "| Operation     | Purpose                         | Typical use                                     |\n",
        "| :------------ | :------------------------------ | :---------------------------------------------- |\n",
        "| `tf.transpose`| Reorder axes                    | Flip rows/columns of inner matrices, move batch dims |\n",
        "| `tf.reshape`  | Change tensor shape             | Flatten, merge, or split dimensions while keeping data order |\n",
        "\n",
        "### 6️⃣ Key Takeaways\n",
        "\n",
        "*   Default `tf.transpose()` reverses all axes → may be confusing for high-rank tensors.\n",
        "*   Use the `perm` argument with `tf.transpose()` for precise axis reordering.\n",
        "*   `tf.reshape()` changes the tensor's view but not the underlying data order; the total number of elements must match the new shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13a303b7"
      },
      "source": [
        "## Tensor Transpose and Reshape in TensorFlow\n",
        "\n",
        "Understanding `tf.transpose()` and `tf.reshape()` is crucial for manipulating tensor dimensions effectively. While both change the arrangement of a tensor's elements, they do so in fundamentally different ways.\n",
        "\n",
        "### 1️⃣ `tf.transpose()` – Reordering Axes\n",
        "\n",
        "*   **Purpose:** Reorders the axes (dimensions) of a tensor according to a specified permutation.\n",
        "*   **Syntax:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31f0181f"
      },
      "source": [
        "tensor_A = tf.constant(np.arange(1,49).reshape(2,2,2,3,2))\n",
        "tensor_A.shape  # Output: (2, 2, 2, 3, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e12cd7ff"
      },
      "source": [
        "tensor_B = tf.transpose(tensor_A)\n",
        "tensor_B.shape  # Output: (2, 3, 2, 2, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9de7f4d8"
      },
      "source": [
        "tensor_C = tf.transpose(tensor_A, perm=[0, 1, 2, 4, 3])\n",
        "tensor_C.shape  # Output: (2, 2, 2, 2, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing Datatype of tensors\n"
      ],
      "metadata": {
        "id": "YsxIQApSh4fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf;\n",
        "tensor_A=tf.constant([[1,2,3],\n",
        "                      [4,5,6]],shape=(2,3),dtype=tf.int32);\n",
        "tensor_B=tf.cast(tensor_A,dtype=tf.int16);\n",
        "tensor_B\n",
        "#cast float32->float16 or float16->int32 or int32->int16(16 bit) and so on"
      ],
      "metadata": {
        "id": "bPmDMkLvh8gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregating Tensors\n",
        "` What “aggregating tensors” means `\n",
        "\n",
        "> Aggregating a tensor means reducing multiple elements into one (or fewer) elements, usually by applying a mathematical operation across one or more axes."
      ],
      "metadata": {
        "id": "eZ7-Dggvjh7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf;\n",
        "import tensorflow_probability as tfp;\n",
        "tensor = tf.constant([[1, 2, 3],\n",
        "                      [4, 5, 6]])\n",
        "\n",
        "# Sum of all elements\n",
        "sumA=tf.reduce_sum(tensor)  # 21\n",
        "\n",
        "# Mean of all elements\n",
        "sumB=tf.reduce_mean(tensor)  # 3.5\n",
        "\n",
        "# Sum along rows (axis=1)\n",
        "sumC=tf.reduce_sum(tensor, axis=1)  # [6, 15]\n",
        "\n",
        "# Sum along columns (axis=0)\n",
        "sumD=tf.reduce_sum(tensor, axis=0)  # [5, 7, 9]\n",
        "max=tf.reduce_max(tensor)\n",
        "min=tf.reduce_min(tensor)\n",
        "variance=tfp.stats.variance(tensor)\n",
        "mean=tf.cast(tf.reduce_mean(tensor),dtype=tf.float32)\n",
        "mean_axis_1=tf.reduce_mean(tensor,axis=0)\n",
        "\n",
        "stddev=tf.math.reduce_std(tf.cast(tensor,dtype=tf.float32),axis=1)\n",
        "sumA,sumB,sumC,sumD,max , min , variance,stddev,mean,mean_axis_1\n",
        "\n"
      ],
      "metadata": {
        "id": "XvCMcj0nj5zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Positional Maximum & Minimum — The Core Idea**\n",
        "\n",
        "> A maximum/minimum tells you what the largest or smallest value is.\n",
        "\n",
        "> A positional maximum/minimum tells you where it is — i.e. its index or position in the tensor.\n",
        "\n",
        "> In TensorFlow, you do this with:\n",
        "\n",
        " `tf.argmax() → position of maximum value`\n",
        "\n",
        " `tf.argmin() → position of minimum value`"
      ],
      "metadata": {
        "id": "s8Xec1YADtkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# Positional Maximum & Minimum in TensorFlow\n",
        "# ================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Simple 1D tensor example\n",
        "# -------------------------------\n",
        "tensor_1d = tf.constant([3, 7, 2, 9, 5])\n",
        "max_pos_1d = tf.argmax(tensor_1d)  # position of maximum\n",
        "min_pos_1d = tf.argmin(tensor_1d)  # position of minimum\n",
        "\n",
        "print(\"1D Tensor:\", tensor_1d.numpy())\n",
        "print(\"Max position:\", max_pos_1d.numpy(), \"Value:\", tensor_1d[max_pos_1d].numpy())\n",
        "print(\"Min position:\", min_pos_1d.numpy(), \"Value:\", tensor_1d[min_pos_1d].numpy())\n",
        "\n",
        "# -------------------------------\n",
        "# 2. 2D tensor example\n",
        "# -------------------------------\n",
        "tensor_2d = tf.constant([[1, 8, 3],\n",
        "                         [7, 2, 5]])\n",
        "# By default, axis=0\n",
        "max_pos_2d = tf.argmax(tensor_2d, axis=0)  # max along rows for each column\n",
        "min_pos_2d = tf.argmin(tensor_2d, axis=0)  # min along rows for each column\n",
        "\n",
        "print(\"\\n2D Tensor:\\n\", tensor_2d.numpy())\n",
        "print(\"Max positions along axis 0:\", max_pos_2d.numpy())\n",
        "print(\"Min positions along axis 0:\", min_pos_2d.numpy())\n",
        "\n",
        "# You can also find max along axis=1 (columns)\n",
        "max_pos_2d_axis1 = tf.argmax(tensor_2d, axis=1)\n",
        "min_pos_2d_axis1 = tf.argmin(tensor_2d, axis=1)\n",
        "\n",
        "print(\"Max positions along axis 1:\", max_pos_2d_axis1.numpy())\n",
        "print(\"Min positions along axis 1:\", min_pos_2d_axis1.numpy())\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Higher-dimensional tensors\n",
        "# -------------------------------\n",
        "# Example 4D tensor\n",
        "g = tf.random.Generator.from_seed(42)\n",
        "tensor_4d = g.truncated_normal(shape=(5, 2, 4, 9), dtype=tf.float32, mean=5.0, stddev=1.0)\n",
        "\n",
        "# Positional maximum along axis 0 (first dimension)\n",
        "max_pos_4d = tf.argmax(tensor_4d, axis=0)\n",
        "min_pos_4d = tf.argmin(tensor_4d, axis=0)\n",
        "\n",
        "print(\"\\n4D Tensor shape:\", tensor_4d.shape)\n",
        "print(\"Positional max shape (axis 0 reduced):\", max_pos_4d.shape)\n",
        "print(\"Positional min shape (axis 0 reduced):\", min_pos_4d.shape)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Understanding what tf.argmax returns\n",
        "# -------------------------------\n",
        "# If tensor has shape (5, 2, 5, 6, 6, 8, 8)\n",
        "# and you call tf.argmax(tensor) without specifying axis, axis=0 is default\n",
        "# result shape will be (2, 5, 6, 6, 8, 8)\n",
        "# Each element is an integer from 0 to 4 (position along axis 0)\n",
        "# It indicates the index where the maximum value occurred along axis 0\n",
        "#\n",
        "# Conceptually:\n",
        "# - Original tensor: 5 stacked 6D \"blocks\"\n",
        "# - tf.argmax -> returns a 6D tensor with the same shape as remaining axes\n",
        "# - Each element tells which of the 5 blocks had the max at that position\n",
        "\n",
        "# Example with smaller tensor for clarity\n",
        "tensor_small = tf.constant([\n",
        "    [[1,2],[3,4]],  # axis 0, index 0\n",
        "    [[5,0],[1,6]],  # axis 0, index 1\n",
        "    [[2,1],[4,0]]   # axis 0, index 2\n",
        "])  # shape (3, 2, 2)\n",
        "\n",
        "max_pos_small = tf.argmax(tensor_small, axis=0)  # reduces axis 0\n",
        "print(\"\\nSmall tensor shape:\", tensor_small.shape)\n",
        "print(\"Max positions along axis 0:\\n\", max_pos_small.numpy())\n",
        "# max_pos_small shape is (2,2)\n",
        "# Each element indicates which slice along axis 0 had the maximum\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Key points to remember\n",
        "# -------------------------------\n",
        "# 1. tf.argmax / tf.argmin returns **positions**, not the values themselves\n",
        "# 2. By default, axis=0; you can specify any axis\n",
        "# 3. Output shape is **original shape without the reduced axis**\n",
        "# 4. For n-dimensional tensors, visualize as:\n",
        "#    - \"axis 0\" = stacked blocks\n",
        "#    - max/min along axis = which block has max/min at each coordinate\n",
        "# 5. For multi-dimensional tensors, each element of result tensor is an **integer index**\n",
        "#    pointing to where the max/min occurred along the reduced axis\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-OVU72FQGQNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One Hot Encoding**"
      ],
      "metadata": {
        "id": "woBBLEatPzef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# One-Hot Encoding in TensorFlow\n",
        "# ================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# -------------------------------\n",
        "# 1. What is one-hot encoding?\n",
        "# -------------------------------\n",
        "# One-hot encoding converts categorical integer labels into a binary vector.\n",
        "# For example, if we have 3 classes (0, 1, 2),\n",
        "# label 1 becomes [0, 1, 0]\n",
        "\n",
        "# Example labels\n",
        "labels = tf.constant([0, 2, 1, 2])\n",
        "\n",
        "print(\"Original labels:\", labels.numpy())\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Using tf.one_hot\n",
        "# -------------------------------\n",
        "# tf.one_hot(indices, depth, on_value=1, off_value=0, axis=-1)\n",
        "# Depth=>no of classes for each label if 3-> 0 1 0 if 4-> 0 1 0 0\n",
        "# Depth >= total number of labels or largest index +1\n",
        "\n",
        "one_hot_labels = tf.one_hot(labels, depth=3)\n",
        "print(\"One-hot encoded labels:\\n\", one_hot_labels.numpy())\n",
        "\n",
        "# Shape explanation:\n",
        "# - Original labels shape: (4,)\n",
        "# - One-hot encoded shape: (4, 3)\n",
        "# Each row corresponds to a label as a one-hot vector\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Specifying axis\n",
        "# -------------------------------\n",
        "# By default, axis=-1, meaning new dimension added at the end\n",
        "# You can insert the one-hot dimension anywhere\n",
        "\n",
        "one_hot_labels_axis0 = tf.one_hot(labels, depth=3, axis=0)\n",
        "print(\"\\nOne-hot with axis=0 shape:\", one_hot_labels_axis0.shape)\n",
        "print(one_hot_labels_axis0.numpy())\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Changing on/off values\n",
        "# -------------------------------\n",
        "one_hot_custom = tf.one_hot(labels, depth=3, on_value=5, off_value=-1)\n",
        "print(\"\\nCustom one-hot values:\\n\", one_hot_custom.numpy())\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Using one-hot with higher-dimensional tensors\n",
        "# -------------------------------\n",
        "# Suppose we have a batch of categorical labels for a mini-batch\n",
        "batch_labels = tf.constant([[0,1], [2,0]])\n",
        "# shape: (2,2)\n",
        "\n",
        "one_hot_batch = tf.one_hot(batch_labels, depth=3)\n",
        "# shape: (2,2,3)\n",
        "print(\"\\nBatch one-hot shape:\", one_hot_batch.shape)\n",
        "print(one_hot_batch.numpy())\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Why is one-hot useful in ML/DL?\n",
        "# -------------------------------\n",
        "# 1. Classification tasks: Output layer of neural networks often predicts class probabilities.\n",
        "# 2. Loss computation: Many loss functions (like categorical cross-entropy) require one-hot labels.\n",
        "# 3. Tensor operations: You can multiply, sum, or mask tensors using one-hot vectors.\n",
        "#\n",
        "# Example: Masking\n",
        "predictions = tf.constant([[0.1, 0.7, 0.2],\n",
        "                           [0.3, 0.2, 0.5]])\n",
        "# Suppose true label is 1\n",
        "mask = tf.one_hot([1,2], depth=3)\n",
        "masked_pred = predictions * mask\n",
        "print(\"\\nMasked predictions:\\n\", masked_pred.numpy())\n"
      ],
      "metadata": {
        "id": "yz_O1pAyP4Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sequeezing a Tensor**"
      ],
      "metadata": {
        "id": "iLoQRo-TSYkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import tensorflow as tf;\n",
        "tensor=tf.random.uniform(shape=(1,2,4,1),dtype=tf.float32);\n",
        "squeezed_tensor=tf.squeeze(tensor);\n",
        "print(tensor)\n",
        "print(squeezed_tensor)\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "pEgjYVkNSdbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **some more math operation with tensor**"
      ],
      "metadata": {
        "id": "I7TriVYqTgqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf;\n",
        "tensor=tf.constant([3,1,2,41,23,42]);\n",
        "# square;\n",
        "squared_tensor=tf.square(tensor)\n",
        "#sqared_root;\n",
        "squared_root_tensor=tf.sqrt(tf.cast(tensor,dtype=tf.float32));\n",
        "#log;\n",
        "log_tensor=tf.math.log(tf.cast(tensor,dtype=tf.float32));\n",
        "#exp;\n",
        "exp_tensor=tf.exp(tf.cast(tensor,dtype=tf.float32));\n",
        "print(tensor)\n",
        "print(squared_tensor)\n",
        "print(squared_root_tensor)\n",
        "print(log_tensor)\n",
        "print(exp_tensor)\n"
      ],
      "metadata": {
        "id": "VTMEJXtfTmwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}